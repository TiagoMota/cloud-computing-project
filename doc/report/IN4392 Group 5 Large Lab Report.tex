\documentclass[a4paper]{IEEEtran}

\usepackage[fleqn]{amsmath}
%\usepackage{amsfonts}
%\usepackage{algorithm}% http://ctan.org/pkg/algorithms
%\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage[]{hyperref}
%\usepackage{verbatim}
%\usepackage{textcomp}
\usepackage{pdflscape}
\usepackage{footnote}

\usepackage{graphicx}
\graphicspath{ {images/} }

\title{CloudOCR: Cloud Computing Large Lab Assignment}
\author{
	\IEEEauthorblockN{Tiago Mota \\}
	\IEEEauthorblockA{Email: neozflux@gmail.com \\}
	\and
	\IEEEauthorblockN{Eddy Bertoluzzo \\}
	\IEEEauthorblockA{Email: eddy.bertoluzzo@gmail.com \\}
	\and
	\IEEEauthorblockN{David Hoepelman\\}
	\IEEEauthorblockA{Email: dhoepelman@gmail.com\\}
	\and
	\IEEEauthorblockN{Course instructor: Alexandru Iosup\\}
	\IEEEauthorblockA{Email: A.Iosup@tudelft.nl \\}
}
\makesavenoteenv{tabular} 

\begin{document}

\maketitle

\begin{abstract}
In this report we introduce a cloud-based system for automatic OCR-conversion of image files, called CloudOCR. The application uses elastic cloud resource provisioning and resource allocation to improve scalability in performance. The experiments conducted, shows that CloudOCR
\end{abstract}

\section{Introduction}


\subsection*{Problem}

WantCloud BV is a library that wants to make its entrance into the 21th century. They digitized their existing book collection and started digitally renting them.

Since this first step was a success, they now want to OCR their digital collection, adding features such as searching through, highlighting, or tagging the texts, in order to offer a better service to their customers.

WantCloud BV does not want to build the infrastructure themselves but would rather lease it from a Cloud Provider. Their main problem is that as a public service they need to keep costs as low as possible.

Their existing (large) selection of books is under little time constraint to OCR, but they do want to be able to prioritize new books and OCR them very quickly.

\subsection*{Proposed solution}
Using Amazon Web Services as the cloud infrastructure provider, we developed CloudOCR, a cloud-based system for the automatic OCR-conversion of images.

CloudOCR scales according to the number of available jobs, load-balances the workload evenly across the rented machines (from now on called workers) and provides fault tolerance in both the master and the workers. 

The system is fully automatic, aside from the submission of jobs to the database and the resolution of faulty jobs.

\subsection*{Report structure}
In section \ref{sec:backgroundonapplication} we will first detail the application and its requirements. In section \ref{sec:systemdesign} we will show the design of the solution. In section \ref{sec:experimentalresults} we will show the experiments we used to evaluate the system.
In the final two sections of this report we will draw conclusions from the evaluations.

\section{Background on Application}
\label{sec:backgroundonapplication}
CloudOCR is based on a existing technology called Tesseract OCR \cite{tesseractocr} that provides OCR services for bitmap images files like ``JPEG'' and ``PNG'' files and extracts their text into plain text files. For simplification our application takes only JPEG image files as input and output a simple text file.

We use the Amazon AWS cloud, specifically the EC2 (virtual machine), S3 (storage) and RDS (relational database) services.

To be able to implement the WantCloud BV's wishes, we have the following requirements
\begin{LaTeXdescription}
	\item[Automation] WantCloud does not have the resources to constantly manage the application. As such the application should perform normally without user intervention.
	\item[Elasticity] WantCloud has a large library to OCR, and as such the application should be able to scale to large numbers of jobs.
	\item[Performance] WantCloud has little time constraints, but the costs must be as low as possible.
	\item[Reliability] Failures or corrupted jobs should be handled gracefully and should not halt the system.
	\item[Monitoring] Worker numbers and job throughput should be monitored.
	\item[Scheduling] The system should make use of Amazon's spot instance system (wherein one bids for system capacity) to keep costs as low as possible.
\end{LaTeXdescription}

\section{System Design}
\label{sec:systemdesign}

\subsection*{Global overview}

CloudOCR uses a master-slave structure. There is one master server which handles all the allocation, scheduling, monitoring and so on. It controls virtual machines (workers) that execute jobs.

One jobs consists of one image file to be processed by Tesseract. Jobs are inserted into an ordered queue which is implemented in a database. The scheduler assigns jobs to workers (at a fixed maximum per worker) and the assignments are also stored in the database.

Workers pull their jobs from the database and start processing them. The monitor retrieves system information which the scheduler, allocation manager and fault manager can use. The allocation manager monitors the number of available jobs and requests spot instances accordingly. The fault manager monitors the system for faults and reinserts uncompleted jobs of failed machines or removes corrupt jobs.

A high-level diagram can be found in Figure \ref{fig_sysarch}.

Our system components are:

\begin{LaTeXdescription}

\item[Front-End] We use simple Java program to store new jobs in the queue. As the insertion consists of only a file upload and a SQL query different (e.g. on-line) front-ends are trivial to make.

\item[Job Queue]
The job queue holds jobs that still need to be processed and meta-information like priority and number of failures. We implemented it using a MySQL database, querying it to get the updated, sorted by priority, job queue.

\item[Job Scheduler]
The scheduler assigns the jobs from the queue to the available workers and includes the logic for \textbf{load-balancing}. A \textbf{greedy on-line load-balancing} algorithm \cite{kleinberg2006} is used to assign these jobs to those workers, until a configurable maximum amount of jobs per worker is reached. The scheduler is executed at regular predefined intervals.

\item[Workers]
A worker exits only to execute jobs. The workers are implemented as Amazon EC2 instances running a common AMI that has the open source OCR engine Tesseract OCR installed.

\item[System Monitor]
The system monitor keeps track of useful information like job status or resource usage, in order to provide this information to other system components like the fault or allocation manager.

\item[Fault Manager]
The fault manager waits for fault information from the monitor and handles accordingly, which means a failed job is resubmitted to the job queue and failing VM's are shutdown (if they are not already). A job that consistently fails is removed from the queue and logged after a configurable amount of failures.

\item[Allocation Manager]
The allocation manager allocates workers according to the number of unprocessed jobs. The minimum amount of on-demand workers is configurable (we set it to 1 to keep a minimum throughput while minimizing costs), otherwise spot instances will be requested if the current price is low enough.

\end{LaTeXdescription}

\subsection*{Resource management}

\begin{LaTeXdescription}
\item[Automation]
Almost all of the implemented system is automated, except submitting new jobs and inspecting faulty jobs (they are removed from the queue).

\item[Elasticity]
The \textbf{Allocation Manager} handle all the worker allocation an deallocation, providing an automatic scaling to the system. The master or database are not scaled.

\item[Performance]
The \textbf{Job Scheduler} is responsible for guaranteeing the system load balance.

\item[Reliability]
Reliability for the workers and jobs is provided by the \textbf{Fault Manager}, which handle all the failures in workers and its jobs. For the master, there is no reliability provided.

\item[Monitoring]
A dedicated monitoring subsystem keep track of the workers and aggregates information for other components to use and can also be logged if necessary.

\end{LaTeXdescription}

\subsection*{System Policies}

\begin{LaTeXdescription}

\item[Job Allocation]
Users can specify a priority to a job, which means that jobs will only be chosen by the scheduler when no higher-priority jobs are available. In order to prevent starvation, jobs with the same priority are scheduled according to time they were submitted, which means first in, first out.

\item[Resource Allocation]
A configurable minimum throughput is maintained and there is no upper limit on allocated resources. If there are enough jobs available and the price of spot instances is low enough, a new instance is allocated.

\end{LaTeXdescription}

\subsection*{Additional System Features}

\begin{LaTeXdescription}
\item[Scheduling]
Our solution focus on efficient scheduling to minimize the total cost. This is done by the Allocation Manager, using spot instances to keep the price as low as possible, while maintaining a configurable minimal throughput.
The user can also configure this price/performance trade-off.

\end{LaTeXdescription}

\section{Experimental results}
\label{sec:experimentalresults}

\subsection{Experimental Setup}

We used Amazon EC2 m1.micro, m1.small and m1.medium instances in all our tests. Specifications can be found in Table \ref{amazoninstancespec}. All instances were running Ubuntu 13.10 (Linux kernel 3.8.0).

The database was provided by Amazon RDS, which uses the same instance specifications but is slightly more expensive. The OS and RDMS were both controlled by Amazon, we know only that a 64-bit Linux was used by checking the mysql status information. We configured RDS to use MySQL 5.6.13.

To keep tests as comparable as possible we used only one job: a 450 KB JPEG file with a 2064x1096 resolution containing 3 scanned book pages (Figure \ref{fig_standardjob}).

We implemented the system as a Java program with seperate entry points for the worker and master. We used sql2o \cite{sql2ocite} to access the database from Java and used SLF4J for loggin \cite{slf4jcite}.

\begin{savenotes}
\begin{table}
\caption{Amazon EC2 instance types}
\label{amazoninstancespec}
\centering
\begin{tabular}{| l | l | l | l | l |}
\hline
Type & CPU\footnote{"One EC2 Compute Unit (ECU) provides the equivalent CPU capacity of a 1.0-1.2 GHz 2007 Opteron or 2007 Xeon processor." \cite{amazonecu}} & RAM & I/O speed & On-demand/Spot price\footnote{"The spot price varies due to demand, this is the minimum spot price which we used as the only price we wanted to rent spot instances."} \\ \hline
Micro & \textless 2 ECU & 0.6 GB & Low & 0.02/0.006 \$/hr \\ \hline
Small & 1 ECU & 1.7 GB & Moderate & 0.065/0.016 \$/hr \\ \hline
Medium & 2 ECU & 3.75 GB & Moderate & 0.13/0.032 \$/hr \\ \hline
\end{tabular}
\end{table}
\end{savenotes}

\subsection{Experiments}

\subsubsection{Database performance}

We use a relation database (MySQL) to store our jobs and assignments. As such the database becomes our most probable bottleneck for scaling. We tested the Amazon RDS database performance with databases containing a different number of jobs and executed the query that is needed to retrieve the jobs for scheduling. To accurately test the bottleneck we retrieved all jobs instead of a number dependent on the number of workers (as the scheduler would do).

We ran the test concurrently from the same VM to the different instance types to make sure environmental factors were minimized. Our own test had previously shown that running the tests concurrently does not have a significant influence on the results. We ran each test 50 times and report the average.

As can be seen from the results in Table \ref{dbperfresults} and Figure \ref{fig_dbperfresults} database overhead only becomes noticeable around 100,000 jobs and only becomes significant starting at a million jobs. As we configured the scheduler to retrieve 10 jobs per active worker this would mean 100,000 workers. We thus conclude that database overhead is not a bottleneck for the scaling of our application.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{"results-database"}
\caption{Average database job retrieval time}
\label{fig_dbperfresults}
\end{figure}

\begin{table}
\caption{Average database job retrieval time}
\label{dbperfresults}
\centering
\begin{tabular}{| l | l | l | l |}
\hline
\# Jobs & Micro & Small & Medium \\ \hline
1000 & 0,004s &	0,003s & 0,007s \\ \hline
10000 & 0,025s & 0,025s & 0,026s \\ \hline
100000 & 0,31s & 0,52s & 0,37s \\ \hline
1000000 & 5,28s & 5,84s & 3,74s \\ \hline
3000000	& 79,46s & 18,87s & 11,61s \\ \hline
\end{tabular}
\end{table}

\subsection{Instance type selection}

To check which instance type would be the best to use for our workers we ran the same job 50 times on the 3 different instance types. To compare the performance we also ran the test on a relatively standard desktop machine (CPU: Intel Core i7 2670QM, RAM: 16GB, OS: Windows 8.0 x64). The results can be seen in Table \ref{tesperfresults} and Figure \ref{fig_tesperfresults}

Our main findings are that the chosen instance type does not greatly influence the price per job if spot instances are used. For production uses we would have chosen the medium instance (and tested other instances to see if they were even cheaper). However to reap the benefits of the free resources provided by Amazon\footnote{Amazon AWS has a "free usage tier" which among other consists of 750 free EC2 micro instance hours and 750 free micro RDS instance hours} we chose to use micro instances in all our development and experiments.

Based on these results we also do \emph{a rough estimate} on the choice for cloud or non-cloud system. The used system costs about \$750 and uses about 200W of power (we used \$0.25/Kwh as energy price). This results in the following calculation for the equivalent cost point of a desktop and cloud solution:
$$
750\$ + 0.2 \cdot 0.25\$ \cdot \frac{\text{\#jobs}}{120} = 0.00037\$ \cdot \text{\#jobs}
$$

Which puts the equivalence well over a million jobs, which would make the desktop run for well over a year. This even excludes other costs like maintance and network infrastructure. All in all this means that a cloud solution is very attractive compared to getting your own machines.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{"results-tesseract"}
\caption{Average job processing time}
\label{fig_tesperfresults}
\end{figure}

\begin{table}
\caption{Average Tesseract job processing time}
\label{tesperfresults}
\centering
\begin{tabular}{| l | l | l |}
\hline
Instance & Processing time & Cost / job (normal/spot) \\ \hline
Micro & 326s & 0,0018 / 0,00054 \$ \\ \hline
Small & 93s & 0,0017 / 0,00041 \$ \\ \hline
Medium & 45s & 0,0016 / 0,00037 \$ \\ \hline
Laptop & 30s & n.a. \\ \hline
\end{tabular}
\end{table}

\subsection{Allocation behavior}

To test whether our application indeed had desired behavior we ran several workloads.

For our first test we configured CloudOCR to only use spot instances. We configured the ideal number of workers to be 10\% the number of assigned jobs, which corresponds to completing all jobs in about an hour (see Table \ref{tesperfresults}). We then started the system with 100 jobs available and added 100 jobs more at the 15 minute mark.
Furthermore we decreased the "de-allocation protection" from 50 minutes to 10 minutes. The de-allocation protection prevents workers from being decomissioned if they have been running fewer than 50 minutes of an hour. This is because Amazon charges for a full hour even if you use only a portion of an hour, so this prevents needless wasting of money. We did this so de-allocation behavior was more apparent. The de-allocation protection also protects too much workers from being killed too fast, which would cause a slowdown when nearly all jobs were completed.

As can be seen clearly from the results our application closely follows the configured allocation. We can also see a lag 

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{"results-allocation-2"}
\caption{Allocation characteristics with workload 1}
\label{fig_tesperfresults}
\end{figure}
 

\section{Discussion}

	\begin{table}
	\caption{Extrapolation of charged-costs}
	\label{extpcosts}
	\centering
	\begin{tabular}{| l | l | l | l |}
	\hline
	Jobs & Micro (normal/spot) & Small (n/s) & Medium (n/s)\\ \hline
	100.000 &  180 / 54 \$ & 170 / 41 \$ &  160 / 41 \$ \\ \hline
	1.000.000 &  1.800 / 540 \$ & 1.700 / 410 \$ &  1.600 / 370 \$ \\ \hline
	10.000.000 &  18.000 / 5.400\$ & 17.000 / 4.100 \$ & 16.000 / 3.700 \$ \\ \hline
	\end{tabular}
	\end{table}	
	
	As future work, CloudOCR could be improved by doin the following:
	\begin{LaTeXdescription}
		\item Carrying out benchmark test on larger instances types
		\item Compilation of information of the monitor, to create statistics
		\item 
	\end{LaTeXdescription}
\section{Conclusion}
In this report, we designed a system based on a IaaS cloud platform for OCR-conversion of ``JPEG'' image files, and evaluated its performance on Amazon Web Services.
Benchmark tests show that

\begin{thebibliography}{9}

\bibitem{kleinberg2006}{
 Algorithm design: Chapter 11, \emph{Kleinberg, Jon and Tardos, Éva}, Pearson/Addison Wesley, Boston (Mass.), ISBN: 0-321-37291-3
 }
 
\bibitem{amazonecu}{
	Amazon EC2 instance description page:  \url{http://aws.amazon.com/ec2/#instance}
}

\bibitem{tesseractocr} {
	Tesseract OCR: \url{https://code.google.com/p/tesseract-ocr/}
}

\bibitem{sql2o} {
	Sql2o: \url{http://www.sql2o.org/}
}

\bibitem{SLF4J} {
	The Simple Logging Facade for Java (SLF4J) : \url{http://www.slf4j.org/}
}

\end{thebibliography}

\appendix{Time spent}

\begin{tabular}{| l | l |}
\hline
total-time & 162 hours \\ \hline
think-time & 19 hours \\ \hline
dev-time & 124 hours \\ \hline
analysis-time & 13 hours \\ \hline
wasted-time & 6 hours \\ \hline
\end{tabular}
\\\ 
\\
The wasted time comes from rewriting a portion of the application because we altered the system design.

Time spent on experiments:
\begin{LaTeXdescription}
\item[Database performance] \ \\
\begin{tabular}{| l | l |}
\hline
total-time & 4 hours \\ \hline
dev-time & 1 hours \\ \hline
setup-time & 3 hours \\ \hline
\end{tabular}
\item[Instance type selection] \ \\
\begin{tabular}{| l | l |}
\hline
total-time & 4 hours \\ \hline
dev-time & 2 hours \\ \hline
setup-time & 2 hours \\ \hline
\end{tabular}
\item[Allocation behavior] \ \\
\begin{tabular}{| l | l |}
\hline
total-time & 2 hours \\ \hline
dev-time & 1 hours \\ \hline
setup-time & 1 hours \\ \hline
\end{tabular}
\\\ 
The experiment itself was very simple to set-up, but we encountered a lot of time to get the system stable enough to test it for multiple hours.
\end{LaTeXdescription}


\begin{landscape}
\appendix

\begin{figure}[h]
\centering
\includegraphics[width=700pt]{"System Architecture 2"}
\caption{System architecture}
\label{fig_sysarch}
\end{figure}
\end{landscape}
\clearpage

\begin{landscape}
\appendix

\begin{figure}[h]
\centering
\includegraphics[width=700pt]{standardfile}
\caption{Standard Job image file}
\label{fig_standardjob}
\end{figure}
\end{landscape}
\clearpage

\end{document}