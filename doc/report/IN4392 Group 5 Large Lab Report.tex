\documentclass[a4paper]{IEEEtran}

\usepackage[fleqn]{amsmath}
%\usepackage{amsfonts}
%\usepackage{algorithm}% http://ctan.org/pkg/algorithms
%\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{hyperref}
%\usepackage{verbatim}
%\usepackage{textcomp}
\usepackage{pdflscape}
\usepackage{footnote}

\usepackage{graphicx}
\graphicspath{ {images/} }

\title{CloudOCR: Cloud Computing Large Lab Assignment}
\author{Tiago Mota, Eddy Bertoluzzo, David Hoepelman}

\makesavenoteenv{tabular} 

\begin{document}

\maketitle

\begin{abstract}

Blah
\end{abstract}

\section{Introduction}


\subsection{Problem}
WantCloud BV with the providing of facilities to run OCR jobs on Tesseract-OCR\footnote{An OCR Engine that was developed at HP Labs between 1985 and 1995... and now at Google.} have been assuring a great amount of financial income and also a preoccupying increase of this service popularity. Since there is only one machine to execute all the jobs submitted by all the users, WantCloud BV verified that most of the system is no longer capable of guaranteeing the job deadlines for all the users, violating it's own Terms-of-Use. They though that is necessary to improve the limitations of one single machine running Tesseract by using a IaaS platform to run multiple instances of the program.

\subsection{Proposed solution}
Using Amazon Web Services as the cloud infrastructure provider, we present CloudOCR, a system capable of OCR an entire digitalized library of papers, returning the output in simple plain text. The system have to guarantee scaling during peak usage and load-balance the across the multiple machines, so to effectively do this, CloudOCR monitors the working machines or instances (hereby called workers) to keep track of the system current load and the need to allocating or deallocating a worker. Also, all the jobs are scheduled to the workers in a way that a minimal workload balance can be maintained.

\subsection{Report structure}
The next section refers to a detailed system design to fully understand the system components, followed by all the experiments and their respective setups. The final two sections of this document will correspond to the results discussion and conclusions.


\section{Background on Application}
CloudOCR is based on a previous working technology called Tesseract-OCR, that provides an OCR engine for images files like ``.jpg'' and ``.png'', turning them into simple text files. Our application takes ``.jpg'' image files as input and output a simple ``.txt'' text file, using EC2 instances and spot instances leased to the Amazon Web Services. Users upload a image file (equivalent to one job) through a simple online front-end which in turn stores the file as a Job in the CloudOCR database. Afterwards, the scheduler will query the database for unassigned jobs and split them among the available workers. At the same time, there is a monitor controlling the system resource usage that provides information to the allocator manager, responsible to allocate and deallocate workers through time, according to the system needs. After a worker finish a job, the output is sent to a user desired storage location.
To be able to implement the WantedCloud BV's successor to the Tesseract service, there are few requirements that need to be addressed:
\begin{itemize}
	\item Automation
	\item Elasticity
	\item Performance
	\item Reliability
	\item Monitoring
\end{itemize}
The next section introduces the system design of CloudOCR and also, explain how did we meet the above requirements.

\section{System Design}

\subsection*{Global overview}
CloudOCR is composed by a master server and working Virtual Machines (hereby VM's only). The master server is the one responsible for all management operations of the system, such as job  scheduling, resource allocation, fault management and so on. The jobs are distributed among the VM's. These are spot instances: impossible to know how long it will take for a new spot instance to be available, but the cost is much lower.
To show the system design, a high-level diagram can be found in Figure \ref{fig_sysarch}.
Our system components are:

\begin{LaTeXdescription}

\item[Front-End]
The online front-end interface where jobs can be submitted by users, and a priority can be assigned. By using a simple Java program, the new jobs are stored on the system database.

\item[Job Queue]
The job queue holds jobs that still need to be processed and meta-information like priority and number of failures. We implemented it using a MySQL database, querying it to get the updated, sorted by priority, job queue.

\item[Job Scheduler]
The scheduler assigns the jobs present on the queue to the available workers and includes the logic for \textbf{load-balancing}. A \textbf{greedy on-line load-balancing} algorithm \cite{kleinberg2006} is used to assign these jobs to those workers, until a predefined maximum amount of jobs per worker is reached and the scheduler is executed at regular predefined intervals.
These predefined values change according to the system policy applied to the CloudOCR application.

\item[Workers]
A worker exits only to execute its jobs. They are implemented as Amazon EC2 instances running a common AMI that has the open source OCR engine installed, Tesseract-OCR.

\item[System Monitor]
The system monitor keeps track of useful information like job status or resource usage, in order to provide this information to other system components like the fault or allocation manager.

\item[Fault Manager]
The fault manager waits for the monitor's fault information and handle it accordingly, which means a failed job is resubmitted to the job queue and failing VM's are shutdown (if not already). A job that consistently fails for a predefined amount of times is logged, instead of resubmitted.

\item[Allocation Manager]
The allocation manager waits for the monitor's resource usage information and new ones are allocated or old ones removed. New spot instances are allocated based on current submitted jobs and when the price is low enough.

\end{LaTeXdescription}

\subsection*{Resource management}

\begin{LaTeXdescription}
\item[Automation]
Almost all of the implemented system is automated, except when submitting new jobs and when dealing with faulty ones, where the job is deleted from the queue.

\item[Elasticity]
The \textbf{Allocation Manager} handle all the new resources (instances) allocation an deallocation, providing an automatic scaling to the system. The master itself is never scaled.

\item[Performance]
The \textbf{Job Scheduler} is responsible for guaranteeing the system load balance.

\item[Reliability]
The system uses a master-slave architecture where the master contains all of the management operations, while the slaves (workers) process jobs. Reliability for the workers and jobs is provided by the \textbf{Fault Manager}, which handle all the failures in VM's and its jobs. For the master, there is no reliability provided

\item[Monitoring]
A dedicated monitoring subsystem keep track of the workers and aggregates information for other components to use and can also be logged if necessary.

\end{LaTeXdescription}

\subsection*{System Policies}

\begin{LaTeXdescription}

\item[Job Allocation]
Users can specify a priority to a job, which means that jobs will only be chosen by the scheduler when no higher-priority jobs are available. In order to prevent starvation, jobs with the same priority are scheduled according to time they were submitted, which means first in, first out.

\item[Resource Allocation]
A configurable minimum throughput is maintained and there is no upper limit on allocated resources. If there are enough jobs available and the price of spot instances is low enough, a new instance is allocated.

\end{LaTeXdescription}

\subsection*{Additional System Features}

\begin{LaTeXdescription}
\item[Scheduling]
Our solution focus on efficient scheduling to minimize the total cost. This is done by the Allocation Manager, using spot instances to keep the price as low as possible, while maintaining a configurable minimal throughput.
The user can also configure this price/performance trade-off.

\end{LaTeXdescription}

\section{Experimantal results}

\subsection{Experimental Setup}

We used Amazon EC2 m1.micro, m1.small and m1.medium instances in all our tests. Specifications can be found in table \ref{amazoninstancespec}. All instances were running Ubuntu 13.10 (Linux kernel 3.8.0).

The database was provided by Amazon RDS, which uses the same instance specifications but is slightly more expensive. The OS and RDMS were both controlled by Amazon, we know only that a 64-bit Linux was used by checking the mysql status information. We configured RDS to use MySQL 5.6.13.

To keep tests as comparable as possible we used only one job: a 450 KB JPEG file with a 2064x1096 resolution containing 3 scanned book pages (Figure \ref{fig_standardjob}).

\begin{savenotes}
\begin{table}
\centering

\begin{tabular}{| l | l | l | l | l |}
\hline
Type & CPU\footnote{"One EC2 Compute Unit (ECU) provides the equivalent CPU capacity of a 1.0-1.2 GHz 2007 Opteron or 2007 Xeon processor." \cite{amazonecu}} & RAM & I/O speed & On-demand/Spot price\footnote{"The spot price varies due to demand, this is the minimum spot price which we used as the only price we wanted to rent spot instances."} \\ \hline
Micro & \textless 2 ECU & 0.6 GB & Low & 0.02/0.006 \$/hr \\ \hline
Small & 1 ECU & 1.7 GB & Moderate & 0.065/0.016 \$/hr \\ \hline
Medium & 2 ECU & 3.75 GB & Moderate & 0.13/0.032 \$/hr \\ \hline
\end{tabular}

\caption{Amazon EC2 instance types}
\label{amazoninstancespec}
\end{table}
\end{savenotes}

\subsection{Experiments}

\subsubsection{Database performance}

We use a relation database (MySQL) to store our jobs and assignments. As such the database becomes our most probable bottleneck for scaling. We tested the Amazon RDS database performance with databases containing a different number of jobs and executed the query that is needed to retrieve the jobs for scheduling. To accurately test the bottleneck we retrieved all jobs instead of a number dependent on the number of workers (as the scheduler would do).

We ran the test concurrently from the same VM to the different instance types to make sure environmental factors were minimized. Our own test had previously shown that running the tests concurrently does not have a significant influence on the results. We ran each test 50 times and report the average.

As can be seen from the results in Table \ref{dbperfresults} and Figure \ref{fig_dbperfresults} database overhead only becomes noticeable around 100,000 jobs and only becomes significant starting at a million jobs. As we configured the scheduler to retrieve 10 jobs per active worker this would mean 100,000 workers. We thus conclude that database overhead is not a bottleneck for the scaling of our application.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{"results-database"}
\caption{Average database job retrieval time}
\label{fig_dbperfresults}
\end{figure}

\begin{table}
\centering

\begin{tabular}{| l | l | l | l |}
\hline
\# Jobs & Micro & Small & Medium \\ \hline
1000 & 0,004s &	0,003s & 0,007s \\ \hline
10000 & 0,025s & 0,025s & 0,026s \\ \hline
100000 & 0,31s & 0,52s & 0,37s \\ \hline
1000000 & 5,28s & 5,84s & 3,74s \\ \hline
3000000	& 79,46s & 18,87s & 11,61s \\ \hline
\end{tabular}

\caption{Average database job retrieval time}
\label{dbperfresults}
\end{table}

\subsection{Instance type selection}

To check which instance type would be the best to use with our application we ran the same job 50 times on the 3 different instance types. To compare the performance we also ran the test on a relatively standard desktop machine (CPU: Intel Core i7 2670QM, RAM: 16GB, OS: Windows 8.0 x64). The results can be seen in Table \ref{tesperfresults} and Figure \ref{fig_tesperfresults}

Our main findings are that the chosen instance type does not greatly influence the price per job if spot instances are used. For production uses we would have chosen the medium instance (and tested other instances to see if they were even cheaper). However to reap the benefits of the free resources provided by Amazon\footnote{Amazon AWS has a "free usage tier" which among other consists of 750 free EC2 micro instance hours and 750 free micro RDS instance hours} we chose to use micro instances in all our development and experiments.

Based on these results we also do a rough estimate on the choice for cloud or non-cloud system. The used system costs about \$750 and uses about 200W of power (we used \$0.25/Kwh as energy price). This results in the following calculation for the equivalent cost point of a desktop and cloud solution:
$$
750\$ + 0.2 \cdot 0.25\$ \cdot \frac{\text{\#jobs}}{120} = 0.0016\$ \cdot \text{\#jobs}
$$

Which puts the equivalence point at about 600,000 - 650,000 jobs, which would make the desktop run for over 200 days. So even excluding important cost factors like maintenance and network and storage infrastructure the cloud-based solution looks like and interesting solution 

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{"results-tessaract"}
\caption{Average job processing time}
\label{fig_tesperfresults}
\end{figure}

\begin{table}
\centering

\begin{tabular}{| l | l | l |}
\hline
Instance & Processing time & Cost / job (normal/spot) \\ \hline
Micro & 326s & 0,00054 / 0,0018 \$ \\ \hline
Small & 93s & 0,00041 / 0,0017 \$ \\ \hline
Medium & 45s & 0,00037 / 0,0016 \$ \\ \hline
Laptop & 30s & n.a. \\ \hline
\end{tabular}

\caption{Average Tessaract job processing time}
\label{tesperfresults}
\end{table}

\subsection{Allocation behavior}

\section{Discussion}

\section{Conclusion}

\begin{thebibliography}{9}

\bibitem{kleinberg2006}{
 Algorithm design: Chapter 11, \emph{Kleinberg, Jon and Tardos, Ã‰va}, Pearson/Addison Wesley, Boston (Mass.), ISBN: 0-321-37291-3
 }
 
\bibitem{amazonecu}{
	Amazon EC2 instance description page:  \url{http://aws.amazon.com/ec2/#instance}
}
\end{thebibliography}

\appendix{Time spent}

\newpage

\begin{landscape}
\appendix

\begin{figure}[h]
\centering
\includegraphics[width=700pt]{"System Architecture 2"}
\caption{System architecture}
\label{fig_sysarch}
\end{figure}
\end{landscape}
\clearpage

\begin{landscape}
\appendix

\begin{figure}[h]
\centering
\includegraphics[width=700pt]{standardfile}
\caption{Standard Job image file}
\label{fig_standardjob}
\end{figure}
\end{landscape}
\clearpage

\end{document}